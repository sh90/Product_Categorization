{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Import packages and download nltk dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shikhatyagi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shikhatyagi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shikhatyagi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikhatyagi/anaconda3/envs/env1/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import libraries for text processing\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# import libraries for building model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of  helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Function to read and subsample data \n",
    "read_and_subsample_data(infile,separator,subsample,restSample,videoSample,musicSample,bookSample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_subsample_data(infile,separator,subsample,restSample,videoSample,musicSample,bookSample):\n",
    "        \n",
    "        train = pd.read_csv(infile,sep=separator)\n",
    "        train.fillna('',inplace=True)\n",
    "        if subsample == True:\n",
    "            train_rest = train[train[\"label\"]==\"rest\"].sample(frac=restSample).reset_index(drop=True)\n",
    "            train_books = train[train[\"label\"]==\"books\"].sample(frac=bookSample).reset_index(drop=True)\n",
    "            train_music = train[train[\"label\"]==\"music\"].sample(frac=musicSample).reset_index(drop=True)\n",
    "            train_music = train[train[\"label\"]==\"videos\"].sample(frac=videoSample).reset_index(drop=True)\n",
    "            \n",
    "            finalTrainData= pd.concat([train_rest,train_books,train_music,train_music])\n",
    "            return finalTrainData\n",
    "        else :\n",
    "            return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Function to keep only keys from  \"additional attribute\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def additional_attributes_key_filter(x):\n",
    "        index=0\n",
    "        tempString = \"\" \n",
    "        for word in x:\n",
    "            if index%2==0:\n",
    "                tempString = tempString + word +\" \"         \n",
    "            index=index+1\n",
    "        return (tempString)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) This function performs below preprocssing on text data - \n",
    "1. Remove stop words \n",
    "2. Remove words of length < 3\n",
    "3. Lemmatize text \n",
    "4. Create dictionary of words appearing in text as key and number of occurances as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_pre_processing(x):\n",
    "        breadcrumbs_split_filter = []\n",
    "        for word in x:\n",
    "            if word.lower() not in stop_words:\n",
    "                if len(word.strip()) >= 3 : \n",
    "                        breadcrumbs_split_filter.append(wordnet_lemmatizer.lemmatize(word.strip()))\n",
    "        return Counter(breadcrumbs_split_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Function to get combined key value pair across all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_key_value_pair(attributeKeys):\n",
    "    dict1 = {}\n",
    "    for index in attributeKeys:\n",
    "        for key in index:\n",
    "            if key in dict1:\n",
    "                dict1[key] = dict1[key]+1\n",
    "            else:\n",
    "                dict1[key] = 1\n",
    "    return dict1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Function to filter infrequent keys from attribute dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_key_value_pair_filter(dict1,value_count_threshold):\n",
    "    dictNew = {}\n",
    "    for key in dict1:\n",
    "        if dict1[key] > value_count_threshold:\n",
    "            dictNew[key] = dict1[key]\n",
    "    return dictNew\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Function to filter infrequent keys from  additionalAttributes and breadcrumbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterKeys(x,dictionary):\n",
    "    newDict = {}\n",
    "    for key in x:\n",
    "        if key in dictionary:\n",
    "            newDict[key] = 1\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603201, 3)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "path = \"/Users/shikhatyagi/Downloads/\"\n",
    "trainFile = \"train_data.csv\"\n",
    "train = read_and_subsample_data(path+trainFile,\",\",False,0.3,1,1,1)\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# get stop word list and remove it from sentences \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "finalData = train[[\"additionalAttributes\",\"breadcrumbs\",\"label\"]]\n",
    "print(finalData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get bag of words from additionalAttributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikhatyagi/anaconda3/envs/env1/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "finalData[\"additionalAttributes_split\"] = finalData[\"additionalAttributes\"].str.split(\";|=\")\n",
    "finalData[\"additionalAttributes_split1\"] = finalData[\"additionalAttributes_split\"].apply(additional_attributes_key_filter)\n",
    "finalData[\"additionalAttributes_split2\"] = finalData[\"additionalAttributes_split1\"].str.replace(':|\\\\?|=|-|\\\\[|\\\\]|\\\\(|\\\\)|\\\\.|>|[0-9]|\"|;|$|$|\\\\*|/|&|,|\\\\s+|\\\\\\'s|\\\\\\'',\" \").str.split(\" \")\n",
    "finalData[\"additionalAttributes_split3\"] = finalData[\"additionalAttributes_split2\"].apply(text_pre_processing)\n",
    "\n",
    "additonalAttributeKeys =finalData[\"additionalAttributes_split3\"].tolist()\n",
    "#create consolidated dictionary \n",
    "additioanl_attribute_key_pair = combined_key_value_pair(additonalAttributeKeys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List conversion complete\n"
     ]
    }
   ],
   "source": [
    "#Filter infrequent keys from consolidated dictionary \n",
    "additioanl_attribute_key_pair_filtered = combined_key_value_pair_filter(additioanl_attribute_key_pair,450)\n",
    "# Filter keys from individual rows\n",
    "finalData[\"additionalAttributes_split4\"] = finalData[\"additionalAttributes_split3\"].apply(filterKeys,dictionary=additioanl_attribute_key_pair_filtered)\n",
    "\n",
    "aATList = finalData[\"additionalAttributes_split4\"].tolist()\n",
    "print(\"List conversion complete\")\n",
    "\n",
    "# Convert list to dataframe where each key will appear as feature\n",
    "aATDF = pd.DataFrame(aATList)\n",
    "# Fill Nan with 0\n",
    "aATDF.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASIN</th>\n",
       "      <th>Actors</th>\n",
       "      <th>Age</th>\n",
       "      <th>Amazon</th>\n",
       "      <th>April</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Arts</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Assembled</th>\n",
       "      <th>Audience</th>\n",
       "      <th>...</th>\n",
       "      <th>format</th>\n",
       "      <th>label</th>\n",
       "      <th>manufacturer_part_number</th>\n",
       "      <th>minute</th>\n",
       "      <th>page</th>\n",
       "      <th>rank</th>\n",
       "      <th>screen_format</th>\n",
       "      <th>size</th>\n",
       "      <th>tape</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ASIN  Actors  Age  Amazon  April  Artist  Arts  Aspect  Assembled  \\\n",
       "0  0.0   0.0     0.0  0.0     0.0    0.0     0.0   0.0     0.0         \n",
       "1  0.0   0.0     0.0  0.0     0.0    0.0     0.0   0.0     0.0         \n",
       "2  0.0   0.0     0.0  0.0     0.0    0.0     0.0   0.0     0.0         \n",
       "3  0.0   0.0     0.0  0.0     0.0    0.0     0.0   0.0     0.0         \n",
       "4  0.0   0.0     0.0  0.0     0.0    0.0     0.0   0.0     0.0         \n",
       "\n",
       "   Audience   ...    format  label  manufacturer_part_number  minute  page  \\\n",
       "0  0.0        ...    0.0     0.0    0.0                       0.0     0.0    \n",
       "1  0.0        ...    0.0     0.0    0.0                       0.0     0.0    \n",
       "2  0.0        ...    0.0     0.0    0.0                       0.0     0.0    \n",
       "3  1.0        ...    0.0     0.0    0.0                       0.0     0.0    \n",
       "4  0.0        ...    0.0     0.0    0.0                       0.0     0.0    \n",
       "\n",
       "   rank  screen_format  size  tape  weight  \n",
       "0  0.0   0.0            0.0   0.0   0.0     \n",
       "1  0.0   0.0            0.0   0.0   0.0     \n",
       "2  0.0   0.0            0.0   0.0   0.0     \n",
       "3  0.0   0.0            0.0   0.0   1.0     \n",
       "4  0.0   0.0            0.0   0.0   0.0     \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aATDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get bag of words from breadcrumbs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalData[\"breadcrumbs_split\"] = finalData[\"breadcrumbs\"].str.replace(':|\\\\?|=|-|\\\\[|\\\\]|\\\\(|\\\\)|\\\\.|>|[0-9]|\"|;|$|$|\\\\*|/|&|,|\\\\s+|\\\\\\'s|\\\\\\'',\" \").str.split(\" \")\n",
    "finalData[\"breadcrumbs_split1\"] = finalData[\"breadcrumbs_split\"].apply(text_pre_processing)\n",
    "breadcrumbsKeys =finalData[\"breadcrumbs_split1\"].tolist()\n",
    "breadcrumbs_key_pair = combined_key_value_pair(breadcrumbsKeys)\n",
    "#Filter infrequent keys from consolidated dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breadcrumbs_key_pair_filtered = combined_key_value_pair_filter(breadcrumbs_key_pair,2550)\n",
    "# Filter keys from individual rows\n",
    "finalData[\"breadcrumbs_split2\"] = finalData[\"breadcrumbs_split1\"].apply(filterKeys,dictionary=breadcrumbs_key_pair_filtered)\n",
    "\n",
    "# Convert breadcrumbs_split1 column to list of dictionary\n",
    "brCList = finalData[\"breadcrumbs_split2\"].tolist()\n",
    "# Convert list to dataframe where each key will appear as feature\n",
    "brCDF = pd.DataFrame(brCList)\n",
    "brCDF.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603201, 219)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brCDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine breadcrumbs and additionalAttributes bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603201, 420)\n"
     ]
    }
   ],
   "source": [
    "different_attribute_list = list(set(aATDF.columns.tolist()) - set(brCDF.columns.tolist()))\n",
    "different_attribute_df = aATDF[different_attribute_list]\n",
    "final_train_df = pd.concat([different_attribute_df,brCDF],axis=1)\n",
    "print(final_train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical class label to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import label encoder to encode categorical label to numeric values\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(finalData[\"label\"].tolist())\n",
    "le1=le.transform(finalData[\"label\"].tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 482560 Validation samples: 120641\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(final_train_df, le1, test_size=0.20, random_state=0)\n",
    "print('Train samples: {} Validation samples: {}'.format(len(x_train), len(x_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Model Training using XGBOOST multi class classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) XGBOOST parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set xgboost parameters\n",
    "params = {}\n",
    "params['objective'] = \"multi:softprob\"\n",
    "params['eta'] = 0.008\n",
    "params['silent'] = True\n",
    "params['max_depth'] = 4\n",
    "params['subsample'] = 0.9\n",
    "params['colsample_bytree'] = 0.9\n",
    "params['colsample_bylevel']=0.9\n",
    "params['min_child_weight']=5\n",
    "params['n_estimators']=500\n",
    "params['learning_rate']=0.01\n",
    "params['n_jobs']=-1\n",
    "params['nthread'] = 24\n",
    "params['num_class'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Fit model using above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n",
       "       colsample_bytree=0.9, eta=0.008, gamma=0, learning_rate=0.01,\n",
       "       max_delta_step=0, max_depth=4, min_child_weight=5, missing=None,\n",
       "       n_estimators=500, n_jobs=-1, nthread=24, num_class=4,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=0.9)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize XGBOOST model\n",
    "model = XGBClassifier(**params)\n",
    "\n",
    "# Train XGBOOST model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Cross validation using GridSearchCV - not used due to longer processing time\n",
    "#clf = GridSearchCV(model, param_grid= params,scoring='accuracy',verbose=2, refit=True,cv =5)\n",
    "#clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print('Best score for data1:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training data 0.999747181698\n",
      "Accuracy score on test data 0.999792773601\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score on training data\", accuracy_score(model.predict(x_train),y_train))\n",
    "print(\"Accuracy score on test data\", accuracy_score(model.predict(x_valid),y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 -  Generate predictions on evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442041, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikhatyagi/anaconda3/envs/env1/lib/python3.6/site-packages/pandas/core/frame.py:2754: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/shikhatyagi/Downloads/\"\n",
    "evaluation_file = \"evaluation.csv\"\n",
    "test = read_and_subsample_data(path+evaluation_file,\",\",False,0.3,1,1,1)\n",
    "test_data = test[[\"additionalAttributes\",\"breadcrumbs\",\"id\"]]\n",
    "test_data.fillna('',inplace=True)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering similar to previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_column = final_train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words creation for additionalAttributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikhatyagi/anaconda3/envs/env1/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_data[\"additionalAttributes_split\"] = test_data[\"additionalAttributes\"].str.split(\";|=\")\n",
    "test_data[\"additionalAttributes_split1\"] = test_data[\"additionalAttributes_split\"].apply(additional_attributes_key_filter)\n",
    "test_data[\"additionalAttributes_split2\"] = test_data[\"additionalAttributes_split1\"].str.replace(':|\\\\?|=|-|\\\\[|\\\\]|\\\\(|\\\\)|\\\\.|>|[0-9]|\"|;|$|$|\\\\*|/|&|,|\\\\s+|\\\\\\'s|\\\\\\'',\" \").str.split(\" \")\n",
    "test_data[\"additionalAttributes_split3\"] = test_data[\"additionalAttributes_split2\"].apply(text_pre_processing)\n",
    "additional_attribute_keys = test_data[\"additionalAttributes_split3\"].tolist()\n",
    "\n",
    "list_new = []\n",
    "for ele in  additional_attribute_keys:\n",
    "    dict_new = {}\n",
    "    for key in ele:\n",
    "        if key  in train_data_column:\n",
    "            dict_new[key] = ele[key]    \n",
    "    list_new.append(dict_new)\n",
    "    \n",
    "test_df1 = pd.DataFrame(list_new)\n",
    "test_df1.fillna(0,inplace=True)\n",
    "\n",
    "print(test_df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words creation for breadcrumbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data[\"breadcrumbs_split\"] = test_data[\"breadcrumbs\"].str.replace(':|\\\\?|=|-|\\\\[|\\\\]|\\\\(|\\\\)|\\\\.|>|[0-9]|\"|;|$|$|\\\\*|/|&|,|\\\\s+|\\\\\\'s|\\\\\\'',\" \").str.split(\" \")\n",
    "test_data[\"breadcrumbs_split1\"] = test_data[\"breadcrumbs_split\"].apply(text_pre_processing)\n",
    "breadcrumbs_keys =test_data[\"breadcrumbs_split1\"].tolist()\n",
    "\n",
    "list_new1 = []\n",
    "for ele in  breadcrumbs_keys:\n",
    "    dict_new = {}\n",
    "    for key in ele:\n",
    "        if key  in train_data_column:\n",
    "            dict_new[key] = ele[key]    \n",
    "    list_new1.append(dict_new)\n",
    "    \n",
    "test_df2 = pd.DataFrame(list_new1)\n",
    "test_df2.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep unique columns in the same order as of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442041, 420)\n"
     ]
    }
   ],
   "source": [
    "different_attribute_list = list(set(test_df1.columns.tolist()) - set(test_df2.columns.tolist()))\n",
    "different_attribute_df = test_df1[different_attribute_list]\n",
    "final_test_df = pd.concat([different_attribute_df,test_df2],axis=1)\n",
    "print(final_test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions on evaluation data using trained mocel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_pred = model.predict(final_test_df[train_data_column])\n",
    "\n",
    "## Write predictions to csv final\n",
    "final_validation = pd.DataFrame()\n",
    "final_validation[\"id\"] = test_data[\"id\"]\n",
    "final_validation[\"class\"] = le.inverse_transform(validation_pred)\n",
    "final_validation.to_csv(\"/Users/shikhatyagi/Downloads/submissions.csv\",sep=\",\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
